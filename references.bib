@misc{conv,
      title={Image Inpainting for Irregular Holes Using Partial Convolutions},
      author={Guilin Liu and Fitsum A. Reda and Kevin J. Shih and Ting-Chun Wang and Andrew Tao and Bryan Catanzaro},
      year={2018},
      eprint={1804.07723},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1804.07723},
}

@misc{pe,
      title={RoFormer: Enhanced Transformer with Rotary Position Embedding},
      author={Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
      year={2023},
      eprint={2104.09864},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2104.09864},
}

@misc{ffw,
      title={GLU Variants Improve Transformer},
      author={Noam Shazeer},
      year={2020},
      eprint={2002.05202},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.05202},
}

@misc{attention,
      title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
      author={Joshua Ainslie and James Lee-Thorp and Michiel de Jong and Yury Zemlyanskiy and Federico Lebrón and Sumit Sanghai},
      year={2023},
      eprint={2305.13245},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.13245},
}

@misc{norm,
      title={Root Mean Square Layer Normalization},
      author={Biao Zhang and Rico Sennrich},
      year={2019},
      eprint={1910.07467},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.07467},
}

@article{fourier,
    title = {Fourier Transform Layer: A proof of work in different training scenarios},
    journal = {Applied Soft Computing},
    volume = {145},
    pages = {110607},
    year = {2023},
    issn = {1568-4946},
    doi = {https://doi.org/10.1016/j.asoc.2023.110607},
    url = {https://www.sciencedirect.com/science/article/pii/S1568494623006257},
    author = {Jakub Zak and Anna Korzynska and Antonina Pater and Lukasz Roszkowiak},
    keywords = {Neural networks, Convolutional neural networks, Fourier transform, Image classification},
    abstract = {In this work two established methods were merged: Fourier Transform and Convolutional Neural Network to classify images in several datasets. Fourier Transform is commonly used in signal processing as well as in image processing. Convolutional Neural Networks are well-suited to image analysis tasks, yet require a lot of processing power and/or time during training process. Fourier Transform Layer is introduced to increase the processing speed without sacrificing accuracy. The motivation is to present and compare an alternative approach to Convolutional Neural Networks, which could reduce the need for GPU training. Models containing only the novel layer were trained to classify images from widely accepted datasets and compared to classification results of simple models containing one convolutional layer. The comparison was performed in terms of test accuracy, Area-Under-Curve and training times. The results showed that, for images of size 128 × 128 and larger, models with the proposed layer reached test accuracy comparable to that reached by convolutional models (accuracy: 96% and 98% respectively), with at least 27% decrease in training time per one epoch on Central Processing Unit.}
}
@article{lasco,
    author = {Morrill, J. and Korendyke, C. and Brueckner, G. and Giovane, F. and Howard, Russell and Koomen, M. and Moses, Dyson and Plunkett, S. and Vourlidas, Angelos and Esfandiari, E. and Rich, Nathan and Wang, Dennis and Thernisien, A. and Lamy, Philippe and Llebaria, Antoine and Biesecker, D. and Michels, D. and Gong, Qinghai and Andrews, M.},
    year = {2006},
    month = {02},
    pages = {331-372},
    title = {Calibration of the Soho/Lasco C3 White Light Coronagraph},
    volume = {233},
    journal = {Solar Physics},
    doi = {10.1007/s11207-006-2058-1}
}

@misc{pino,
    title={Physics-Informed Neural Operator for Learning Partial Differential Equations},
    author={Zongyi Li and Hongkai Zheng and Nikola Kovachki and David Jin and Haoxuan Chen and Burigede Liu and Kamyar Azizzadenesheli and Anima Anandkumar},
    year={2023},
    eprint={2111.03794},
    archivePrefix={arXiv},
    primaryClass={cs.LG},
    url={https://arxiv.org/abs/2111.03794},
}
